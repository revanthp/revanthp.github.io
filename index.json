[{"content":"Large Language Models (LLMs) provide different outputs for the same input despite setting temperature as zero - which makes sampling deterministic eliminating variance in output. This has been attributed to two main sources:\nfloating point arithmetic, and Parallelism in GPU. Together they tend to make floating point arithmetic non-associative.\na+(b+c)≠(a+b)+c a + (b + c) \\neq (a + b) + c a+(b+c)=(a+b)+cWe can think of this as GPU cores in parallel do computation, which results in nondeterministic sequence of values. And when we do floating point math on these, it results in different values.\nHorace He1 from thinking labs argues that the non-associativity of floating point arithmetic is is technically true in theory, but not the main cause of nondeterminism during inference because modern inference kernels use deterministic vectorized reductions (not atomics). Atomics allow threads to update the same memory location concurrently, causing random ordering. He argues that inference using highly optimized libraries like PyTorch do not use atomic operations and rather apply vectorized operations. This eliminates the problem of non-associative arithmetic. He instead points out to batch size being the true culprit of non-determinism of LLMs during inference time. He points out that during inference / forward pass, non-determinism is introduced by varying batching size depending upon the load on the GPU.\nOutputs can be deterministic for a given batch yet different across different batch sizes.\nHe identifies 3 main culprits that are impacted by batch-size: RMSnorm, attention and matmul.\nLet us look at the code for batch RMSnorm.\n# x: [batch_size, hidden_dim] # weight: [hidden_dim] def rms_norm(x, weight): return x * torch.rsqrt(torch.mean(x ** 2, dim=-1, keepdim=True)) * weight Let us decode what is happening here. We understand at a high level that every row in X is processed by a thread block for data parallelism. But in reality, GPU kernel considers the GPU load (batch size) and if the load is less, it splits some of these rows to other thread block to make full utilization of the GPU kernel. This optimization results in non deterministic floating point accumulation order.\nThe fix is to use batch-invariant kernels that always reduce each row on a single thread block, even if it leaves some GPU cores idle.\nHorace He notes this penalty for fixed batch-size is usually low when your real batch size is close to the fixed size, but can be substantial when it’s much smaller.\nIn short, LLM nondeterminism at temperature 0 comes not from random sampling or floating-point chaos, but from batch-size–dependent parallel reduction strategies. Making kernels batch-invariant restores determinism at the cost of some throughput.\nReferences: He, Horace and Thinking Machines Lab, Defeating Nondeterminism in LLM Inference, Thinking Machines Lab: Connectionism, Sep 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://revanthp.github.io/posts/addressing-non-determinisim-in-llm-outputs/","summary":"\u003cp\u003eLarge Language Models (LLMs) provide different outputs for the same input despite setting temperature as zero - which makes sampling deterministic eliminating variance in output. This has been attributed to two main sources:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003efloating point arithmetic, and\u003c/li\u003e\n\u003cli\u003eParallelism in GPU.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eTogether they tend to make floating point arithmetic non-associative.\u003c/p\u003e\n\u003cspan class=\"katex-display\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003ea\u003c/mi\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003eb\u003c/mi\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmi\u003ec\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo mathvariant=\"normal\"\u003e≠\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003ea\u003c/mi\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmi\u003eb\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmi\u003ec\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\na + (b + c) \\neq (a + b) + c\n\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ea\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e+\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eb\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e+\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ec\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e\u003cspan class=\"mrel\"\u003e\u003cspan class=\"mord vbox\"\u003e\u003cspan class=\"thinbox\"\u003e\u003cspan class=\"rlap\"\u003e\u003cspan class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"\u003e\u003c/span\u003e\u003cspan class=\"inner\"\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mrel\"\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"fix\"\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ea\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e+\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eb\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e+\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.4306em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ec\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cp\u003eWe can think of this as GPU cores in parallel do computation, which results in nondeterministic sequence of values. And when we do floating point math on these, it results in different values.\u003c/p\u003e","title":"Addressing non-determinism in LLM outputs"}]