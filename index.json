[{"content":"In online applications customer journey identification and conversion tracking are essential to grow any business. The customer journey starts with a customer reaching the landing page and browsing / engaging with the content before the customer makes a purchase/logs in. It is important to tie this anonymous behaviour with the existing customer behaviour to\nPersonalize content like recommend products that are useful to the customer. Understand if the landing page conversion to take an action like purchase are meaningful. Identify and filter out bots from customer behavioural data. Below is a simple and quick fingerprinting system design.\nDefinitions: Even though we are calling session level data, it is actually event payload and cookie information.\nSetup Let us start with an internal customer-session database. We will use the session information as a unique idenfier for every single user session on the online application. This gives us a one-to-many mapping. Each customer_id can be mapped to multiple session_ids. Each session can be characterized by data from the browser, clicks, etc.\nGoals We want to build a system that classifies a given session id to an existing group of customer ids or identify it as a new customer. It is okay to classify two sessions that belong to a customer id as two differnt customers. But we do not want to 2 customer_ids to be mapped to the same cutomer id.\nSystem requirements:\n100K user base Online user fingerprinting system latency 20 msec. Problem Formulation There are a couple of ways we can formulate this solution, let us go thorugh one by one.\nMulti-class Classification: We could say each customer_id is one class and train an algorithm on this data to predict the class the session_id belong to. This approach has a few drawbacks:\nWe would have huge number of classes to begin with, in the order of 100K customers to begin with and number of records per session would be very sparse around 10 or so on an average resulting in highly sparse dataset - we would run into the curse of dimensionality early on. With every new customer, we would have to constantly be training every day or every few hours to update the classes. This is not a very scalabe system. Clustering: We could use an algorithm like Kmeans or DBSCAN or some other algrithm. With this we will be able to map existing customers but new customers with multiple sessions run into challenges where we would have to retrain the algorithm with updating training set very frequently.\nEmbedding based Proximity Search: The main idea here is that session information of a customer are closer in a higher dimensional space compared to a session from a different customers. Now the goal is to learn the higher dimensional space vector representation and distance threshold to identify sessions from the same customer to different customers. This has the following advantages:\nCustomer level data sparsity: This is not an issue during inference since we are just mapping from higher dimension sparse vector space to a lower dimension dense vector representation.\nNew customer identification: A new customer will have higher distance from neighboring sessions compared to sessions belonging to the same customer. This comes from the problem formulation itself.\nML System Design Stage 1/2: Embedding System Design: The goal here is to ensure that the vector representation of the session_id has to be closer for sessions for the same customer but farther for sessions from different customers. This is a solved problem and used frequently in facial recognition. We can use triplet loss function which returns lower distance for sessions that belong to the same customer and greater distance for sessions that belong to different customers. The triplet loss function is defined as below.\nLoss=max(0,d(a,p)−d(a,n)+α) Loss = max(0, d(a, p)-d(a, n)+\\alpha) Loss=max(0,d(a,p)−d(a,n)+α)Where,\nd - Distance metric\na - Anchor sample\np - Positive sample\nn - Negative sample\nalpha - Margin\nStage 2/2: Classifier threshold Now that we have the embeddings for any incoming session, we just need to calcuate the cosine distance between any two records to classify them as belong to the same customer or different customers. We can select this threshold as a heuristic that maximizes the evaluation metric. This can be tuned by using a Receiver Operating Characteristic (ROC) curve.\nPutting it all together Evaluation:\nOverall Evaluation Metric: Macro Precision\nTrue Positive (TP): sessions belonging to the correct customer_id\nTrue Negative (TN): single session for a new customer.\nFalse Positive (FP): single session mapped to wrong customer_id\nFalse Negative (FN): session belonging to a customer (with multiple sessions) mapped to new customer.\nGiven that we have 50 metrics with categorical columns containing 2-3 categories in each, we can assume we will have roughly about 100 - 150 one hot encoded columns. We can now train a neural network that takes these as input columns and returns an embedding of size sqrt of 100 to sqrt 150, so roughly an embedding of 10 - 13. We will consider the embeddding size as a hyper-paramter to tune.\nNeural Network Setup:\ninput nodes: 100-150\noutput nodes: 10-12\nhidden layers: 2, 3, 4\nLoss function for training neural network: Triplet loss with cosine distance\nAdd a normalization layer before cosine computation.\nHyperparatmers to tune:\nhidden layers: 2, 3, 4\noutput nodes: 10, 11, 12\ndropout: 10, 20, 30\nTriplet loss alpha: 0.2, 0.3, 0.5, 0.7\nDataset:\nTraining dataset size:\n12K unique customers with 10 records each at minimum. More sessions per customer the better. Uniform distribution across device types (iOS vs Android), brower types (Safari vs Firefox vs Chrome) Filter records from Browers and devices that are currently supported and not deprecated anytime soon. This ensures that the patterns learnt are consistent with what is seen today. 1K customers with 10 records each chosen for validation dataset from training. We need ensure that the distribution between training and validation is consistent. Test set:\n1K customers with 10 records each chosen for validation dataset from training. We need ensure that the distribution between training and test is consistent. Computing and looking up cosine distance across 100K - 1M records each time will be expensive and unnecessary. Instead we can simply use efficient implementation of this using Hierarchical Navigable Small World (HNSW) for Approximate Nearest neighbor seach. An efficient implementation of this can be found in FAISS python library (from Meta).\n","permalink":"https://revanthp.github.io/posts/designing-fingerprinting-systems/","summary":"\u003cp\u003eIn online applications customer journey identification and conversion tracking are essential to grow any business. The customer journey starts with a customer reaching the landing page and browsing / engaging with the content before the customer makes a purchase/logs in. It is important to tie this anonymous behaviour with the existing customer behaviour to\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003ePersonalize content like recommend products that are useful to the customer.\u003c/li\u003e\n\u003cli\u003eUnderstand if the landing page conversion to take an action like purchase are meaningful.\u003c/li\u003e\n\u003cli\u003eIdentify and filter out bots from customer behavioural data.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eBelow is a simple and quick fingerprinting system design.\u003c/p\u003e","title":"Designing Fingerprinting Systems"},{"content":"Large Language Models (LLMs) provide different outputs for the same input despite setting temperature as zero - which makes sampling deterministic eliminating variance in output. This has been attributed to two main sources:\nfloating point arithmetic, and Parallelism in GPU. Together they tend to make floating point arithmetic non-associative.\na+(b+c)≠(a+b)+c a + (b + c) \\neq (a + b) + c a+(b+c)=(a+b)+cWe can think of this as GPU cores in parallel do computation, which results in nondeterministic sequence of values. And when we do floating point math on these, it results in different values.\nHorace He1 from thinking labs argues that the non-associativity of floating point arithmetic is is technically true in theory, but not the main cause of nondeterminism during inference because modern inference kernels use deterministic vectorized reductions (not atomics). Atomics allow threads to update the same memory location concurrently, causing random ordering. He argues that inference using highly optimized libraries like PyTorch do not use atomic operations and rather apply vectorized operations. This eliminates the problem of non-associative arithmetic. He instead points out to batch size being the true culprit of non-determinism of LLMs during inference time. He points out that during inference / forward pass, non-determinism is introduced by varying batching size depending upon the load on the GPU.\nOutputs can be deterministic for a given batch yet different across different batch sizes.\nHe identifies 3 main culprits that are impacted by batch-size: RMSnorm, attention and matmul.\nLet us look at the code for batch RMSnorm.\n# x: [batch_size, hidden_dim] # weight: [hidden_dim] def rms_norm(x, weight): return x * torch.rsqrt(torch.mean(x ** 2, dim=-1, keepdim=True)) * weight Let us decode what is happening here. We understand at a high level that every row in X is processed by a thread block for data parallelism. But in reality, GPU kernel considers the GPU load (batch size) and if the load is less, it splits some of these rows to other thread block to make full utilization of the GPU kernel. This optimization results in non deterministic floating point accumulation order.\nThe fix is to use batch-invariant kernels that always reduce each row on a single thread block, even if it leaves some GPU cores idle.\nHorace He notes this penalty for fixed batch-size is usually low when your real batch size is close to the fixed size, but can be substantial when it’s much smaller.\nIn short, LLM nondeterminism at temperature 0 comes not from random sampling or floating-point chaos, but from batch-size–dependent parallel reduction strategies. Making kernels batch-invariant restores determinism at the cost of some throughput.\nReferences: He, Horace and Thinking Machines Lab, Defeating Nondeterminism in LLM Inference, Thinking Machines Lab: Connectionism, Sep 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://revanthp.github.io/posts/addressing-non-determinisim-in-llm-outputs/","summary":"\u003cp\u003eLarge Language Models (LLMs) provide different outputs for the same input despite setting temperature as zero - which makes sampling deterministic eliminating variance in output. This has been attributed to two main sources:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003efloating point arithmetic, and\u003c/li\u003e\n\u003cli\u003eParallelism in GPU.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eTogether they tend to make floating point arithmetic non-associative.\u003c/p\u003e\n\u003cspan class=\"katex-display\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003ea\u003c/mi\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003eb\u003c/mi\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmi\u003ec\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo mathvariant=\"normal\"\u003e≠\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003ea\u003c/mi\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmi\u003eb\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmi\u003ec\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\na + (b + c) \\neq (a + b) + c\n\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ea\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e+\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eb\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e+\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ec\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e\u003cspan class=\"mrel\"\u003e\u003cspan class=\"mord vbox\"\u003e\u003cspan class=\"thinbox\"\u003e\u003cspan class=\"rlap\"\u003e\u003cspan class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"\u003e\u003c/span\u003e\u003cspan class=\"inner\"\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mrel\"\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"fix\"\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ea\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e+\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eb\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e+\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.4306em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ec\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cp\u003eWe can think of this as GPU cores in parallel do computation, which results in nondeterministic sequence of values. And when we do floating point math on these, it results in different values.\u003c/p\u003e","title":"Addressing non-determinism in LLM outputs"}]