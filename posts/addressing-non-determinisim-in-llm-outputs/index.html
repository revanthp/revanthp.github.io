<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Addressing non-determinism in LLM outputs | Revanth's blog</title><meta name=keywords content><meta name=description content="Large Language Models (LLMs) provide different outputs for the same input despite setting temperature as zero - which makes sampling deterministic eliminating variance in output. This has been attributed to two main sources:

floating point arithmetic, and
Parallelism in GPU.

Together they tend to make floating point arithmetic non-associative.
a+(b+c)≠(a+b)+c
a + (b + c) \neq (a + b) + c
a+(b+c)=(a+b)+cWe can think of this as GPU cores in parallel do computation, which results in nondeterministic sequence of values. And when we do floating point math on these, it results in different values."><meta name=author content="Revanth Pentyala"><link rel=canonical href=https://revanthp.github.io/posts/addressing-non-determinisim-in-llm-outputs/><link crossorigin=anonymous href=https://revanthp.github.io/assets/css/stylesheet.a8501b4e1544bf104a4e2647be64df3637f95b6ac25a4298047d87fe0eacffa1.css integrity="sha256-qFAbThVEvxBKTiZHvmTfNjf5W2rCWkKYBH2H/g6s/6E=" rel="preload stylesheet" as=style><link rel=icon href=https://revanthp.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://revanthp.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://revanthp.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://revanthp.github.io/apple-touch-icon.png><link rel=mask-icon href=https://revanthp.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://revanthp.github.io/posts/addressing-non-determinisim-in-llm-outputs/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=preconnect href=https://cdn.jsdelivr.net crossorigin><link rel=preconnect href=https://gc.zgo.at crossorigin><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=preload as=style href=https://revanthp.github.io/scss/custom.min.be36c2cbcca2bc0d647f885b04906b79936d2b044488b50ddc981645f22dc584.css onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://revanthp.github.io/scss/custom.min.be36c2cbcca2bc0d647f885b04906b79936d2b044488b50ddc981645f22dc584.css></noscript><script defer src=https://revanthp.github.io/js/performance.min.eab2011b5e402f6423e6f864df14369b701f1b07719273a0c43d5b082b8ac680.js></script><style>.critical-content{font-display:swap}img{height:auto;max-width:100%}#theme-toggle{will-change:transform}@media(prefers-reduced-motion:reduce){*,*::before,*::after{animation-duration:.01ms !important;animation-iteration-count:1 !important;transition-duration:.01ms !important}}</style><link rel=preload as=style href=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css></noscript><script defer data-goatcounter=https://fawkes2014.goatcounter.com/count src=https://gc.zgo.at/count.js></script><link rel=dns-prefetch href=//www.google-analytics.com><meta http-equiv=X-UA-Compatible content="IE=edge"><meta http-equiv=X-Content-Type-Options content="nosniff"><meta http-equiv=Referrer-Policy content="strict-origin-when-cross-origin"><meta http-equiv=Accept-CH content="DPR, Viewport-Width, Width"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=dns-prefetch href=//cdn.jsdelivr.net><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><meta http-equiv=Referrer-Policy content="strict-origin-when-cross-origin"><meta http-equiv=X-Content-Type-Options content="nosniff"><meta http-equiv=X-XSS-Protection content="1; mode=block"><meta http-equiv=Accept-CH content="DPR, Viewport-Width, Width"><meta http-equiv=Accept-CH content="DPR, Viewport-Width, Width"><meta property="og:title" content="Addressing non-determinism in LLM outputs"><meta property="og:description" content="Large Language Models (LLMs) provide different outputs for the same input despite setting temperature as zero - which makes sampling deterministic eliminating variance in output. This has been attributed to two main sources:

floating point arithmetic, and
Parallelism in GPU.

Together they tend to make floating point arithmetic non-associative.
a+(b+c)≠(a+b)+c
a + (b + c) \neq (a + b) + c
a+(b+c)=(a+b)+cWe can think of this as GPU cores in parallel do computation, which results in nondeterministic sequence of values. And when we do floating point math on these, it results in different values."><meta property="og:type" content="article"><meta property="og:url" content="https://revanthp.github.io/posts/addressing-non-determinisim-in-llm-outputs/"><meta property="og:image" content="https://revanthp.github.io/favicon.ico"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-08-21T20:12:25-05:00"><meta property="article:modified_time" content="2025-10-01T15:57:06-07:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://revanthp.github.io/favicon.ico"><meta name=twitter:title content="Addressing non-determinism in LLM outputs"><meta name=twitter:description content="Large Language Models (LLMs) provide different outputs for the same input despite setting temperature as zero - which makes sampling deterministic eliminating variance in output. This has been attributed to two main sources:

floating point arithmetic, and
Parallelism in GPU.

Together they tend to make floating point arithmetic non-associative.
a+(b+c)≠(a+b)+c
a + (b + c) \neq (a + b) + c
a+(b+c)=(a+b)+cWe can think of this as GPU cores in parallel do computation, which results in nondeterministic sequence of values. And when we do floating point math on these, it results in different values."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"\"Posts\"","item":"\"https://revanthp.github.io/posts/\""},{"@type":"ListItem","position":2,"name":"\"Addressing non-determinism in LLM outputs\"","item":"\"https://revanthp.github.io/posts/addressing-non-determinisim-in-llm-outputs/\""}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"\"Addressing non-determinism in LLM outputs\"","name":"\"Addressing non-determinism in LLM outputs\"","description":"\"Large Language Models (LLMs) provide different outputs for the same input despite setting temperature as zero - which makes sampling deterministic eliminating variance in output. This has been attributed to two main sources:\\nfloating point arithmetic, and Parallelism in GPU. Together they tend to make floating point arithmetic non-associative.\\na+(b+c)≠(a+b)+c a + (b + c) \\\\neq (a + b) + c a+(b+c)=(a+b)+cWe can think of this as GPU cores in parallel do computation, which results in nondeterministic sequence of values. And when we do floating point math on these, it results in different values.\\n\"","keywords":[],"articleBody":"\"Large Language Models (LLMs) provide different outputs for the same input despite setting temperature as zero - which makes sampling deterministic eliminating variance in output. This has been attributed to two main sources:\\nfloating point arithmetic, and Parallelism in GPU. Together they tend to make floating point arithmetic non-associative.\\na+(b+c)≠(a+b)+c a + (b + c) \\\\neq (a + b) + c a+(b+c)=(a+b)+cWe can think of this as GPU cores in parallel do computation, which results in nondeterministic sequence of values. And when we do floating point math on these, it results in different values.\\nHorace He1 from thinking labs argues that the non-associativity of floating point arithmetic is is technically true in theory, but not the main cause of nondeterminism during inference because modern inference kernels use deterministic vectorized reductions (not atomics). Atomics allow threads to update the same memory location concurrently, causing random ordering. He argues that inference using highly optimized libraries like PyTorch do not use atomic operations and rather apply vectorized operations. This eliminates the problem of non-associative arithmetic. He instead points out to batch size being the true culprit of non-determinism of LLMs during inference time. He points out that during inference / forward pass, non-determinism is introduced by varying batching size depending upon the load on the GPU.\\nOutputs can be deterministic for a given batch yet different across different batch sizes.\\nHe identifies 3 main culprits that are impacted by batch-size: RMSnorm, attention and matmul.\\nLet us look at the code for batch RMSnorm.\\n# x: [batch_size, hidden_dim] # weight: [hidden_dim] def rms_norm(x, weight): return x * torch.rsqrt(torch.mean(x ** 2, dim=-1, keepdim=True)) * weight Let us decode what is happening here. We understand at a high level that every row in X is processed by a thread block for data parallelism. But in reality, GPU kernel considers the GPU load (batch size) and if the load is less, it splits some of these rows to other thread block to make full utilization of the GPU kernel. This optimization results in non deterministic floating point accumulation order.\\nThe fix is to use batch-invariant kernels that always reduce each row on a single thread block, even if it leaves some GPU cores idle.\\nHorace He notes this penalty for fixed batch-size is usually low when your real batch size is close to the fixed size, but can be substantial when it’s much smaller.\\nIn short, LLM nondeterminism at temperature 0 comes not from random sampling or floating-point chaos, but from batch-size–dependent parallel reduction strategies. Making kernels batch-invariant restores determinism at the cost of some throughput.\\nReferences: He, Horace and Thinking Machines Lab, Defeating Nondeterminism in LLM Inference, Thinking Machines Lab: Connectionism, Sep 2025. ↩︎\\n\"","wordCount":"447","inLanguage":"\"en\"","image":"\"https://revanthp.github.io/favicon.ico\"","datePublished":"\"2024-08-21T20:12:25-05:00\"","dateModified":"\"2025-10-01T15:57:06-07:00\"","author":{"@type":"Person","name":"\"Revanth Pentyala\"","url":"\"https://www.linkedin.com/in/revanthpentyala/\""},"mainEntityOfPage":{"@type":"WebPage","@id":"\"https://revanthp.github.io/posts/addressing-non-determinisim-in-llm-outputs/\""},"publisher":{"@type":"Organization","name":"\"Revanth's blog\"","logo":{"@type":"ImageObject","url":"\"https://revanthp.github.io/favicon.ico\""}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://revanthp.github.io/ accesskey=h title="Revanth's blog (Alt + H)">Revanth's blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://revanthp.github.io/posts title=Posts><span>Posts</span></a></li><li><a href=https://revanthp.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://revanthp.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://revanthp.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Addressing non-determinism in LLM outputs</h1><div class=post-meta><span title='2024-08-21 20:12:25 -0500 -0500'>August 21, 2024</span>&nbsp;·&nbsp;Revanth Pentyala</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#references>References:</a></li></ul></nav></div></details></div><div class=post-content><p>Large Language Models (LLMs) provide different outputs for the same input despite setting temperature as zero - which makes sampling deterministic eliminating variance in output. This has been attributed to two main sources:</p><ol><li>floating point arithmetic, and</li><li>Parallelism in GPU.</li></ol><p>Together they tend to make floating point arithmetic non-associative.</p><span class=katex-display><span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>a</mi><mo>+</mo><mo stretchy="false">(</mo><mi>b</mi><mo>+</mo><mi>c</mi><mo stretchy="false">)</mo><mo mathvariant="normal">≠</mo><mo stretchy="false">(</mo><mi>a</mi><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo><mo>+</mo><mi>c</mi></mrow><annotation encoding="application/x-tex">
a + (b + c) \neq (a + b) + c
</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6667em;vertical-align:-.0833em></span><span class="mord mathnormal">a</span><span class=mspace style=margin-right:.2222em></span><span class=mbin>+</span><span class=mspace style=margin-right:.2222em></span></span><span class=base><span class=strut style=height:1em;vertical-align:-.25em></span><span class=mopen>(</span><span class="mord mathnormal">b</span><span class=mspace style=margin-right:.2222em></span><span class=mbin>+</span><span class=mspace style=margin-right:.2222em></span></span><span class=base><span class=strut style=height:1em;vertical-align:-.25em></span><span class="mord mathnormal">c</span><span class=mclose>)</span><span class=mspace style=margin-right:.2778em></span><span class=mrel><span class=mrel><span class="mord vbox"><span class=thinbox><span class=rlap><span class=strut style=height:.8889em;vertical-align:-.1944em></span><span class=inner><span class=mord><span class=mrel></span></span></span><span class=fix></span></span></span></span></span><span class=mrel>=</span></span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:1em;vertical-align:-.25em></span><span class=mopen>(</span><span class="mord mathnormal">a</span><span class=mspace style=margin-right:.2222em></span><span class=mbin>+</span><span class=mspace style=margin-right:.2222em></span></span><span class=base><span class=strut style=height:1em;vertical-align:-.25em></span><span class="mord mathnormal">b</span><span class=mclose>)</span><span class=mspace style=margin-right:.2222em></span><span class=mbin>+</span><span class=mspace style=margin-right:.2222em></span></span><span class=base><span class=strut style=height:.4306em></span><span class="mord mathnormal">c</span></span></span></span></span><p>We can think of this as GPU cores in parallel do computation, which results in nondeterministic sequence of values. And when we do floating point math on these, it results in different values.</p><p>Horace He<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> from thinking labs argues that the non-associativity of floating point arithmetic is is technically true in theory, but <strong>not the main cause of nondeterminism during inference</strong> because modern inference kernels use deterministic vectorized reductions (not atomics). Atomics allow threads to update the same memory location concurrently, causing random ordering. He argues that inference using highly optimized libraries like PyTorch do not use atomic operations and rather apply vectorized operations. This eliminates the problem of non-associative arithmetic. He instead points out to batch size being the true culprit of non-determinism of LLMs during inference time. He points out that during inference / forward pass, non-determinism is introduced by varying batching size depending upon the load on the GPU.</p><blockquote><p>Outputs can be <strong>deterministic for a given batch</strong> yet <strong>different across different batch sizes</strong>.</p></blockquote><p>He identifies 3 main culprits that are impacted by batch-size: RMSnorm, attention and matmul.</p><p>Let us look at the code for batch RMSnorm.</p><pre tabindex=0><code># x: [batch_size, hidden_dim]
# weight: [hidden_dim]
def rms_norm(x, weight):
    return x * torch.rsqrt(torch.mean(x ** 2, dim=-1, keepdim=True)) * weight
</code></pre><p>Let us decode what is happening here. We understand at a high level that every row in X is processed by a thread block for data parallelism. But in reality, GPU kernel considers the GPU load (batch size) and if the load is less, it splits some of these rows to other thread block to make full utilization of the GPU kernel. This optimization results in non deterministic floating point accumulation order.</p><blockquote><p>The fix is to use <strong>batch-invariant kernels</strong> that always reduce each row on a single thread block, even if it leaves some GPU cores idle.</p></blockquote><p>Horace He notes this penalty for fixed batch-size is usually <strong>low when your real batch size is close to the fixed size</strong>, but <strong>can be substantial when it’s much smaller</strong>.</p><p>In short, LLM nondeterminism at temperature 0 comes not from random sampling or floating-point chaos, but from <strong>batch-size–dependent parallel reduction strategies</strong>. Making kernels <strong>batch-invariant</strong> restores determinism at the cost of some throughput.</p><h2 id=references>References:<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>He, Horace and Thinking Machines Lab, <a href=https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/>Defeating Nondeterminism in LLM Inference</a>, Thinking Machines Lab: Connectionism, Sep 2025.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://revanthp.github.io/>Revanth's blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>