<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Revanth&#39;s blog</title>
    <link>https://revanthp.github.io/</link>
    <description>Recent content on Revanth&#39;s blog</description>
    <image>
      <title>Revanth&#39;s blog</title>
      <url>https://revanthp.github.io/favicon.ico</url>
      <link>https://revanthp.github.io/favicon.ico</link>
    </image>
    <generator>Hugo -- 0.150.0</generator>
    <language>en</language>
    <lastBuildDate>Fri, 12 Sep 2025 12:29:18 -0700</lastBuildDate>
    <atom:link href="https://revanthp.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Addressing non-determinism in LLM outputs</title>
      <link>https://revanthp.github.io/posts/addressing-non-determinisim-in-llm-outputs/</link>
      <pubDate>Wed, 21 Aug 2024 20:12:25 -0500</pubDate>
      <guid>https://revanthp.github.io/posts/addressing-non-determinisim-in-llm-outputs/</guid>
      <description>&lt;p&gt;Large Language Models (LLMs) provide different outputs for the same input despite setting temperature as zero - which makes sampling deterministic eliminating variance in output. This has been attributed to two main sources:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;floating point arithmetic, and&lt;/li&gt;
&lt;li&gt;Parallelism in GPU.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Together they tend to make floating point arithmetic non-associative.&lt;/p&gt;
$$
a + (b + c) \neq (a + b) + c
$$&lt;p&gt;We can think of this as GPU cores in parallel do computation, which results in nondeterministic sequence of values. And when we do floating point math on these, it results in different values.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
